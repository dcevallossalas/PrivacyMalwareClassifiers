# Binary Classification Logistic Regression
lr = c(0.998,0.998,0.998,0.998,0.999,0.997,0.99,0.998,0.999,0.999)
mean(lr)
sd(lr)
# Global configuration
knitr::opts_chunk$set(echo = TRUE)
# Libraries
library(ggplot2)
library(viridis)
library(kableExtra)
library(dplyr)
library(sur)
library(qqplotr)
library(DescTools)
library(phia)
library(emmeans)
library(multcompView)
library(multcomp, quietly = TRUE)
# Viridis palette
palette1 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
palette2 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
# Decision Tree
dt_nsm = c(0.713,0.709,0.714,0.716,0.715,0.715,0.714,0.713,0.711,0.719)
dt_sm = c(0.713,0.709,0.715,0.708,0.715,0.712,0.714,0.712,0.711,0.719)
mean(dt_nsm)
sd(dt_nsm)
mean(dt_sm)
sd(dt_sm)
fligner.test(x=list(dt_nsm,dt_sm))
wilcox.test(dt_nsm,dt_sm, alternative="greater")
# Random Forest
rf_nsm = c(0.755,0.752,0.744,0.746,0.761,0.756,0.761,0.750,0.742,0.760)
rf_sm = c(0.756,0.738,0.748,0.733,0.759,0.751,0.746,0.750,0.732,0.757)
mean(rf_nsm)
sd(rf_nsm)
mean(rf_sm)
sd(rf_sm)
fligner.test(x=list(rf_nsm,rf_sm))
wilcox.test(rf_nsm,rf_sm, alternative="greater")
# SVC
svc_nsm = c(0.700,0.696,0.701,0.698,0.687,0.689,0.702,0.690,0.699,0.693)
svc_sm = c(0.698,0.699,0.691,0.698,0.695,0.701,0.695,0.678,0.679,0.698)
mean(svc_nsm)
sd(svc_nsm)
mean(svc_sm)
sd(svc_sm)
fligner.test(x=list(svc_nsm,svc_sm))
wilcox.test(svc_nsm,svc_sm, alternative="greater")
# LDA
lda_nsm = c(0.724,0.721,0.720,0.725,0.721,0.721,0.719,0.715,0.729,0.725)
lda_sm = c(0.732,0.726,0.728,0.730,0.732,0.726,0.729,0.726,0.735,0.734)
mean(lda_nsm)
sd(lda_nsm)
mean(lda_sm)
sd(lda_sm)
fligner.test(x=list(lda_nsm,lda_sm))
wilcox.test(lda_nsm,lda_sm, alternative="less")
# Gaussian Naive Bayes
nb_nsm = c(0.678,0.693,0.686,0.680,0.681,0.680,0.681,0.679,0.679,0.697)
nb_sm = c(0.679,0.697,0.686,0.681,0.682,0.682,0.683,0.679,0.679,0.705)
mean(nb_nsm)
sd(nb_nsm)
mean(nb_sm)
sd(nb_sm)
fligner.test(x=list(nb_nsm,nb_sm))
wilcox.test(nb_nsm,nb_sm, alternative="less")
# Deep Neural Network (DNN)
dnn_nsm = c(0.763,0.732,0.758,0.751,0.753,0.728,0.761,0.773,0.756,0.762)
dnn_sm = c(0.772,0.766,0.760,0.769,0.782,0.775,0.779,0.778,0.763,0.743)
mean(dnn_nsm)
sd(dnn_nsm)
mean(dnn_sm)
sd(dnn_sm)
fligner.test(x=list(dnn_nsm,dnn_sm))
wilcox.test(dnn_nsm,dnn_sm, alternative="less")
# Decision Tree and DNN
wilcox.test(dt_sm,dnn_sm, alternative="less")
# Random Forest and DNN
wilcox.test(rf_sm,dnn_sm, alternative="less")
# SVC and DNN
wilcox.test(svc_sm,dnn_sm, alternative="less")
# LDA and DNN
wilcox.test(lda_sm,dnn_sm, alternative="less")
# Gaussian Naive Bayes and DNN
wilcox.test(nb_sm,dnn_sm, alternative="less")
labels = factor(c(rep("DT",10),rep("RF",10),rep("SVM",10),rep("LDA",10),rep("GNB",10),rep("DNN",10)),levels=c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))
scores = c(dt_sm,rf_sm,svc_sm,lda_sm,nb_sm,dnn_sm)
df  <- data.frame(
labels  = labels,
scores = scores
)
picture = ggplot(df, aes(x = labels, y = scores, fill=labels)) +
geom_boxplot()+
stat_boxplot(geom="errorbar", width=0.3)+
labs(x="Classifier", y = "Accuracy", color="")+
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_manual(values = palette1,labels = c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))+
scale_colour_manual(values = palette1) +
guides(fill=guide_legend(title="Classifier"))+
scale_y_continuous(limits=c(0.675,0.7755))
# Global configuration
knitr::opts_chunk$set(echo = TRUE)
# Libraries
library(ggplot2)
library(viridis)
library(kableExtra)
library(dplyr)
library(sur)
library(qqplotr)
library(DescTools)
library(phia)
library(emmeans)
library(multcompView)
library(multcomp, quietly = TRUE)
# Viridis palette
palette1 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
palette2 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
# Decision Tree
dt_nsm = c(0.713,0.709,0.714,0.716,0.715,0.715,0.714,0.713,0.711,0.719)
dt_sm = c(0.713,0.709,0.715,0.708,0.715,0.712,0.714,0.712,0.711,0.719)
mean(dt_nsm)
sd(dt_nsm)
mean(dt_sm)
sd(dt_sm)
fligner.test(x=list(dt_nsm,dt_sm))
wilcox.test(dt_nsm,dt_sm, alternative="greater")
# Random Forest
rf_nsm = c(0.755,0.752,0.744,0.746,0.761,0.756,0.761,0.750,0.742,0.760)
rf_sm = c(0.756,0.738,0.748,0.733,0.759,0.751,0.746,0.750,0.732,0.757)
mean(rf_nsm)
sd(rf_nsm)
mean(rf_sm)
sd(rf_sm)
fligner.test(x=list(rf_nsm,rf_sm))
wilcox.test(rf_nsm,rf_sm, alternative="greater")
# SVC
svc_nsm = c(0.700,0.696,0.701,0.698,0.687,0.689,0.702,0.690,0.699,0.693)
svc_sm = c(0.698,0.699,0.691,0.698,0.695,0.701,0.695,0.678,0.679,0.698)
mean(svc_nsm)
sd(svc_nsm)
mean(svc_sm)
sd(svc_sm)
fligner.test(x=list(svc_nsm,svc_sm))
wilcox.test(svc_nsm,svc_sm, alternative="greater")
# LDA
lda_nsm = c(0.724,0.721,0.720,0.725,0.721,0.721,0.719,0.715,0.729,0.725)
lda_sm = c(0.732,0.726,0.728,0.730,0.732,0.726,0.729,0.726,0.735,0.734)
mean(lda_nsm)
sd(lda_nsm)
mean(lda_sm)
sd(lda_sm)
fligner.test(x=list(lda_nsm,lda_sm))
wilcox.test(lda_nsm,lda_sm, alternative="less")
# Gaussian Naive Bayes
nb_nsm = c(0.678,0.693,0.686,0.680,0.681,0.680,0.681,0.679,0.679,0.697)
nb_sm = c(0.679,0.697,0.686,0.681,0.682,0.682,0.683,0.679,0.679,0.705)
mean(nb_nsm)
sd(nb_nsm)
mean(nb_sm)
sd(nb_sm)
fligner.test(x=list(nb_nsm,nb_sm))
wilcox.test(nb_nsm,nb_sm, alternative="less")
# Deep Neural Network (DNN)
dnn_nsm = c(0.763,0.732,0.758,0.751,0.753,0.728,0.761,0.773,0.756,0.762)
dnn_sm = c(0.772,0.766,0.760,0.769,0.782,0.775,0.779,0.778,0.763,0.743)
mean(dnn_nsm)
sd(dnn_nsm)
mean(dnn_sm)
sd(dnn_sm)
fligner.test(x=list(dnn_nsm,dnn_sm))
wilcox.test(dnn_nsm,dnn_sm, alternative="less")
# Decision Tree and DNN
wilcox.test(dt_sm,dnn_sm, alternative="less")
# Random Forest and DNN
wilcox.test(rf_sm,dnn_sm, alternative="less")
# SVC and DNN
wilcox.test(svc_sm,dnn_sm, alternative="less")
# LDA and DNN
wilcox.test(lda_sm,dnn_sm, alternative="less")
# Gaussian Naive Bayes and DNN
wilcox.test(nb_sm,dnn_sm, alternative="less")
labels = factor(c(rep("DT",10),rep("RF",10),rep("SVM",10),rep("LDA",10),rep("GNB",10),rep("DNN",10)),levels=c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))
scores = c(dt_sm,rf_sm,svc_sm,lda_sm,nb_sm,dnn_sm)
df  <- data.frame(
labels  = labels,
scores = scores
)
picture = ggplot(df, aes(x = labels, y = scores, fill=labels)) +
geom_boxplot()+
stat_boxplot(geom="errorbar", width=0.3)+
labs(x="Classifier", y = "Accuracy", color="")+
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_manual(values = palette1,labels = c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))+
scale_colour_manual(values = palette1) +
guides(fill=guide_legend(title="Classifier"))+
scale_y_continuous(limits=c(0.675,0.7755))
# Global configuration
knitr::opts_chunk$set(echo = TRUE)
# Libraries
library(ggplot2)
library(viridis)
library(kableExtra)
library(dplyr)
library(sur)
library(qqplotr)
library(DescTools)
library(phia)
library(emmeans)
library(multcompView)
library(multcomp, quietly = TRUE)
library(ggradar)
# Viridis palette
palette1 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
palette2 <- viridis(6, alpha= 1, begin = 0, end = 1, direction = 1, option="viridis")
# Decision Tree
dt_nsm = c(0.593,0.592,0.590,0.587,0.588,0.587,0.590,0.593,0.591, 0.590)
dt_sm = c(0.591,0.592,0.587,0.593,0.588,0.592,0.589,0.588,0.594,0.592)
mean(dt_nsm)
sd(dt_nsm)
mean(dt_sm)
sd(dt_sm)
fligner.test(dt_nsm,dt_sm)
wilcox.test(dt_nsm,dt_sm,alternative="less")
# Random Forest
rf_nsm = c(0.621,0.623,0.613,0.620,0.616,0.611,0.617,0.619,0.622,0.618)
rf_sm = c(0.620,0.622,0.615,0.619,0.615,0.615,0.617,0.620,0.623,0.621)
mean(rf_nsm)
sd(rf_nsm)
mean(rf_sm)
sd(rf_sm)
fligner.test(rf_nsm,rf_sm)
wilcox.test(rf_nsm,rf_sm,alternative="less")
# SVC
svc_nsm = c(0.550,0.549,0.548,0.551,0.552,0.546,0.564,0.555,0.541,0.548)
svc_sm = c(0.541,0.546,0.545,0.542,0.549,0.545,0.545,0.553,0.541,0.546)
mean(svc_nsm)
sd(svc_nsm)
mean(svc_sm)
sd(svc_sm)
fligner.test(svc_nsm,svc_sm)
wilcox.test(svc_nsm,svc_sm, alternative="greater")
# LDA
lda_nsm = c(0.598,0.600,0.598,0.587,0.599,0.595,0.599,0.595,0.595,0.593)
lda_sm = c(0.599,0.603,0.601,0.595,0.602,0.600,0.602,0.596,0.599,0.600)
mean(lda_nsm)
sd(lda_nsm)
mean(lda_sm)
sd(lda_sm)
fligner.test(lda_nsm,lda_sm)
wilcox.test(lda_nsm,lda_sm,alternative="less")
# Gaussian Naive Bayes
nb_nsm = c(0.565,0.564,0.564,0.563,0.568,0.563,0.569,0.565,0.565,0.566)
nb_sm = c(0.568,0.564,0.565,0.563,0.570,0.565,0.571,0.565,0.566,0.566)
mean(nb_nsm)
sd(nb_nsm)
mean(nb_sm)
sd(nb_sm)
fligner.test(nb_nsm,nb_sm)
wilcox.test(nb_nsm,nb_sm, alternative="less")
# Deep Neuronal Network (DNN)
dnn_nsm = c(0.621,0.637,0.625,0.613,0.610,0.607,0.620,0.622,0.612,0.636)
dnn_sm = c(0.710,0.682,0.677,0.698,0.709,0.704,0.705,0.672,0.662,0.611)
mean(dnn_nsm)
sd(dnn_nsm)
mean(dnn_sm)
sd(dnn_sm)
fligner.test(x=list(dnn_nsm,dnn_sm))
wilcox.test(dnn_nsm,dnn_sm, alternative="less")
# Decision Tree and DNN
wilcox.test(dt_sm,dnn_sm, alternative="less")
# Random Forest and DNN
wilcox.test(rf_sm,dnn_sm, alternative="less")
# SVC and DNN
wilcox.test(svc_sm,dnn_sm, alternative="less")
# LDA and DNN
wilcox.test(lda_sm,dnn_sm, alternative="less")
# Gaussian Naive Bayes and DNN
wilcox.test(nb_sm,dnn_sm, alternative="less")
labels = factor(c(rep("DT",10),rep("RF",10),rep("SVM",10),rep("LDA",10),rep("GNB",10),rep("DNN",10)),levels=c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))
scores = c(dt_sm,rf_sm,svc_sm,lda_sm,nb_sm,dnn_sm)
df  <- data.frame(
labels  = labels,
scores = scores
)
picture = ggplot(df, aes(x = labels, y = scores, fill=labels)) +
geom_boxplot()+
stat_boxplot(geom="errorbar", width=0.3)+
labs(x="Classifier", y = "Accuracy", color="")+
theme(plot.title = element_text(hjust = 0.5)) +
scale_fill_manual(values = palette1,labels = c("DT", "RF", "SVM", "LDA", "GNB", "DNN"))+
scale_colour_manual(values = palette1) +
guides(fill=guide_legend(title="Classifier"))+
scale_y_continuous(limits=c(0.525,0.700))
suppressPackageStartupMessages(library(dplyr))
df  <- data.frame(
Classifier  = factor(c("DT","RF","SVM","LDA","GNB","DNN"),levels=c("DT", "RF", "SVM", "LDA", "GNB", "DNN")),
Precision = c(0.669,0.693,0.560,0.614,0.627,0.712),
Recall = c(0.592,0.619,0.541,0.598,0.568,0.679),
F1score = c(0.574,0.614,0.525,0.598,0.544,0.682),
Accuracy = c(0.592,0.619,0.541,0.598,0.568,0.679),
AUC = c(0.853,0.888,0.830,0.859,0.833,0.934)
)
ggradar(df, values.radar = c(0.6,0.7,1),
base.size = 10,
font.radar = "times new roman",
background.circle.colour = "white",
axis.line.colour = "black",
gridline.min.colour = "black",
gridline.mid.colour = "black",
gridline.max.colour = "black",
group.line.width = 0.25,
group.point.size = 2,
group.colours = palette1)
